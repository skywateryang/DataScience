{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datawhale零基础入门NLP赛事 - Task1 赛题理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.赛题理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个比赛的目标是要求对给定的新闻文本进行类别分类，数据集经整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据，这是一个非常经典的NLP文本分类问题。\n",
    "为了预防选手人工标注测试集的情况，比赛数据的文本按照字符级别进行了匿名处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.知识点总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1文本分类的基本步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个完整的文本分类问题分为以下几步：\n",
    "1. Problem definition and solution approach\n",
    "2. Creation of the initial dataset\n",
    "3. Exploratory Data Analysis\n",
    "4. Feature Engineering\n",
    "5. Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Problem definition and solution approach\n",
    "问题定义：基于新闻文本的多分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Creation of the initial dataset\n",
    "使用pandas导入数据集，数据集共两列，标签和文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Exploratory Data Analysis\n",
    "可以从两方面探索数据集：<br />\n",
    "1）分类是否均衡，如果不均衡可能要考虑采用过采样或欠采样的预处理方法 <br />\n",
    "2）文本长度的分布情况，对于不同标签，文本长度是否具有显著差异，这会影响到NLP模型的使用以及特征构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Feature Engineering\n",
    "特征工程是整个比赛的重要环节，下面将重点分析，完整的特征工程分为几步，text representation, text cleaning, label coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text representation\n",
    "在文本分类中需要创建特征（列），不同的特征创造方法会创建不同类型的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 词袋模型 （Bag of Words methods），不考虑语序 <br />\n",
    "    - Word Count Vectors  <br />\n",
    "        - 以单词词频作为特征<br />\n",
    "    - TF–IDF Vectors <br />\n",
    "        - 考虑到单词在整个语料库中的相对重要性，一个单词在整个语料库中出现的频率越高，则它的权重也越低，说明不是这段语料的特有词汇<br />\n",
    "        - TF (term frequency)，需要标准化解决不同语料长度不等的情况<br />\n",
    "        - IDF(inverse document frequency)<br />\n",
    "        - TFIDF = TF * log(N/DF)<br />\n",
    "- 考虑语序的模型 Word Embedding 词嵌入   <br />\n",
    "    - word2vec<br />\n",
    "        - 每个词都可以用一个dense的向量来表示，这会使得具有相似用法的词在向量表示上更加接近<br />\n",
    "        - 原理上这是基于被称为distributional hypothesis的语义学理论，认为一个词语的词义可以从它在语料库中的分布得到体现，即语境类似的词理应具有相似的含义<br />\n",
    "        - 包含两种方法 Continuous Bag-of-Words, or CBOW model （上下文预测中间词汇）和Continuous Skip-Gram Model（中间词汇预测上下文）<br />\n",
    "    - FastText<br />\n",
    "       -  facebook开源的词嵌入模型，优点是适合大型数据+高效的训练速度<br />\n",
    "        - 和Word2Vec 中的 CBOW 模型类似，区别在于fasttext是预测标签而不是预测中间词<br />\n",
    "        - fasttext的特点1：使用了一个分层分类器（而非扁平式架构），不同的类别被整合进树形结构中，而非 list。当处理不均衡分类情况时，频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，使得计算效率更高。<br />\n",
    "        - fasttext的特点2：加入了 N-gram 特征，提高准确率。<br />\n",
    "- Text based or NLP based features<br />\n",
    "    - text based 如单词长度，词汇密度<br />\n",
    "    - NLP based 如词性标注的频率分布<br />\n",
    "- Topic modeling<br />\n",
    "    - Latent Dirichlet Allocation 考虑了基于单个词和多个词的TFIDF模型<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text cleaning\n",
    "- 特殊符号 清除<br />\n",
    "- 标点 清除<br />\n",
    "- 大小写 统一<br />\n",
    "- Stemming（词根化） or Lemmatization（词形还原）<br />\n",
    "- 停用词 清除<br />\n",
    "\n",
    "#### 对category做Label coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Predictive Models\n",
    "赛题官方使用f1_score的macro模式进行结果评估。对于多分类问题，f1_score有几种模式，查阅sklearn官方文档的说明，<br />\n",
    "'micro': 对所有分类加和计算<br />\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.<br />\n",
    "\n",
    "'macro': 对每个分类单独计算取平均<br />\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.<br />\n",
    "\n",
    "'weighted':对每个分类单独计算**加权**平均<br />\n",
    "Calculate metrics for each label, and find their average weighted by support<br />\n",
    "\n",
    "简单来说，macro模式会考虑到分类不均衡情况的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2从word2vec到BERT的算法演变之路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec：最早期的Word2vec方法存在的问题是无法分辨一词多义的现象，所以这种方法产生的词向量在面对多义词时会表现为多种意思的叠加态。<br />\n",
    "\n",
    "- Embedding from Language Models（ELMO) : 将静态的预训练过程变为可动态调整的，即在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。ELMO的模式被称为Feature-based Pre-Training，给下游提供的是每个单词的特征形式。<br />\n",
    "\n",
    "- Generative Pre-Training（GPT）：使用transformer提取特征，而不是RNN，可以做快速的并行计算，但是GPT采取的是单向语言模型，即不考虑下文的信息。GPT的模式被称为pre-tune + fine-tune，需要把任务的网络结构改造成和GPT的网络结构一样的，利用第一步预训练好的参数初始化GPT的网络结构，然后对网络参数进行Fine-tuning。<br />\n",
    "\n",
    "- Bidirectional Encoder Representation from Transformers（BERT），在GPT的基础上使用双向语言模型。<br />\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
